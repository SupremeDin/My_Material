DNN
=====

- **全连接**
- 如果是L-layer NN，则包含了L-1个**隐藏层**，最后的L层是**输出层**
- **可以将Logistic Regression看做是仅含有一层神经元的单层的神经网络**
- **why DNN**? (相对于NN而言)(**优点**)
	- 隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高
	- 特征复杂度与神经网络层数成正相关
	- 能够减少神经元个数,从而减少计算量 (相对于只使用单个隐藏层,使用神经元的个数减少)
	
- 神经网络中的**参数（parameters）**和**超参数（hyperparameters）**
	- 参数:和
	- 超参数: 学习速率，训练迭代次数N，神经网络层数L，各层神经元个数，激活函数等
	- **之所以叫做==超参数==的原因是它们决定了==参数==和的值**
	
- **越深越好**?问题的产生
	- 随着神经网络层数的加深，优化函数越来越容易陷入局部最优解(即过拟合)，并且这个“陷阱”越来越偏离真正的全局最优。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，“梯度消失”现象更加严重。
		- 容易陷入局部最优解? why?
		- “梯度消失”现象更加严重?why?
		- 上面两个问题学习链接[link](http://blog.sina.com.cn/s/blog_15f0112800102wojj.html) 