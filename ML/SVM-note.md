SVM
=====
[学习链接](http://www.blogjava.net/zhenandaci/category/31868.html) 

- 支持向量机方法是建立在统计学习理论的==VC 维理论==和==结构风险最小原理==基础上的，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度，Accuracy）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折衷，以期获得最好的推广能力[14]（或称泛化能力）。
	- VC维：问题的复杂度 一个问题VC维越高 就越复杂 SVM在解决问题的时候，和样本的维数是无关的。所以适合解决`文本分类问题`
	- VC维越大：泛化能力越差。（原因是：VC维越大，虽然能精确的记住每一个样本，但对样本之外的数据却一律的分类错误）
	- 结构风险：
		- 真实误差：一定是不知道的
		- 经验风险：分类器结果 与 label 之间的差值 （以前的机器学习方法都把经验风险最小化作为努力的目标，过拟合 不能解决）
		- 泛化误差界：真实风险 = 经验风险（可测）+置信风险（不可测）
		- 置信风险
			- 样本数量 （负相关）
			- 分类函数的VC维 （正相关）
	- 统计学习的目标 从经验风险最小化------------>结构风险最小化（即 经验风险+置信风险的和最小）
	- 小样本 
	- 非线性 （SVM擅长应付 数据样本线性不可分的情况，通过松弛变量、核函数技术来实现）

- （二）线性分类器

	- 线性分类器（感知机）
	- 线性可分 线性不可分
	- 线性函数？（一维空间：点 二维空间：直线 三维空间 平面）不关注空间的维数，这种线性函数还有一个统一的名称----超平面 **超平面是一个线性函数**
	- 分类间隔 ：定义一个样本点到某个超平面的间隔 目标：这个间隔越大越好
	- ||w|| 范数 （常用2阶范数来表示长度）
	- 几何间隔：点到超平面的欧式距离 几何间隔越大，误分次数的上界就越小。因此 **最大化几何间隔**成了我们训练阶段的目标。

- （四）线性分类器的求解

	- 间隔      y(wx+b)=|g(x)|
	- 几何间隔     |g(x)| / ||w||
	- 最大化几何间隔 <=> 最小化||w||
	- 常用的求解方法是 ：固定间隔（例如：另 |g(x)| = 1）,寻求最小的||w||,　使得　几何间隔　最大。
	- 目标函数　min ||w||   <=> min ||w||^/2 
	- 约束条件　yi[(w·xi)+b]≥1 (i=1,2,…,l)　即约束条件是：在正负类分别都在分类面两端的情况下。

- （五）线性分类器的求解２

	 - 规划
	 - 可行域　。。。。
	 - 凸二次规划 (一定有解，严谨证实)
	 - 在高等数学中　我们会求解
	 	- **不带任何约束条件**的优化问题　（求极值　即求导数另其为０）
	 	- **带等式约束**的优化问题　（条件极值：　拉格朗日乘数法　构造拉格朗日函数，从而消去约束函数）
	 - 带**不等式约束**的优化问题呢？
	
- (六) 线性分类器的求解　－－　问题的转化　直观角度
- (七) 为啥需要核函数  (解决线性不可分)
	- 问题抛出:之前讲的是用**线性分类器**来对**线性可分的样本**做处理
	- 线性不可分的样本 不能用 线性分类器来求解,求不出
	- 寻求:某种方法,将**线性不可分**的数据变得**线性可分**?
		- **解决线性不可分问题的基本思路——向高维空间转化，使其变得线性可分**
	- 是否能有这样一种函数K(w,x),他接受低维空间的输入值，却能算出高维空间的内积值<w’,x’>？  它被称作核函数（核，kernel），而且还不止一个，事实上，只要是满足了`Mercer条件`的函数，都可以作为核函数。
	- 核函数的基本作用就是接受两个低维空间里的向量，能够计算出经过某个变换后在高维空间里的向量内积值。
	- 引出两个问题:
		- 既然有很多的核函数，针对具体问题该怎么选择？ (径向基核函数)
		- 如果使用核函数向高维空间映射后，问题仍然是线性不可分的，那怎么办？
- (八) 松弛变量
	- **仅有少数点线性不可分**叫做`“近似线性可分”的问题 `
	- 这样的少数点 是噪声(我们应该忽略他)
	- `硬间隔的分类`
	- 软间隔分类器 增加松弛变量(非负)
		- 只有“离群点”才有松弛变量,所有没离群的点松弛变量都等于0
		- 松弛变量的值实际上标示出了对应的点到底离群有多远，值越大，点就越远
		- 惩罚因子C决定了你有多重视离群点带来的损失(类似正则化系数的作用)
		- 尽管加上了松弛变量,但这个优化问题的记得步骤并没有发生改变
- **松弛变量 **与** 核函数 **都是解决线性不可分的方法,有何异同?
	- 使用的一般过程是这样的: 刚开始无论怎样确定分类平面,都有大量的**离群点**
		- 1.先使用**核函数**向高维空间映射一下, 结果是**离群点**大量减少,但比原始空间里更加接近线性可分的状态.(**近似线性可分状态**)
		- 再用**松弛变量**处理剩下的少数的**离群点**

- **支持向量机SVM** 最常用的形式: **使用了核函数的软间隔线性分类法**

未完待续~