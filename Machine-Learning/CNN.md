CNN
=====
学习链接[CNN](https://blog.csdn.net/m0_37490039/article/details/79378143) 

- **why CNN**
	- 以一个例子引出为啥要学习**CNN**
		- 回想之前的例子,使用DNN进行0~9手写体识别,我们的输入:5000张20\*20的灰度图片. 当时是这样处理的:将20*20展开成一个400维的向量,作为输入.但是这样,虽然也学习到了图像的特征(使用DNN进行图像识别,最后能够正确分类!说明也是学到了图像的某些空间结构) 但是图像本身是二维,其各个像素点之间是具有空间中的某种关系的,展开成向量之后,相当于忽略了这个关系.
	- 有没有好一点模型,可以考虑到(图像)这种像素点之间的关系呢?  **CNN**
- **有监督**  样本集由形如**(输入向量,理想输出向量)**的向量对构成
- **CNN** ==3个基本思想==
	- **Local receptive fileds** (局部感受野)
		- 局部区域/窗口/卷积核/过滤器
	- **Shared weights**(权值共享)
		- 又称`稀疏连接`
		- 由于权值共享,一个 **窗口/卷积核/过滤器**只能学到一个特征!形成一张特征图
		- 图像识别的时候,只学到一个特征是不够的----->所以需要多个窗口.
		- 不同的窗口之间 权值不共享:即w,b的值是不一样的
		- 一个w 一般为5\*5或(3\*3)等奇数相乘 用25+1或(9+1)个参数来学1个特征.(+1是因为b)
		- 一层**hiddenlayer**可以有多张特征图(由多个窗口分别产生)
	- **Pooling** (池化)
		- **目的 : 为了简化卷积层的输出**
		- 带来的**好处 : 大大减少了我们学到的特征值**,从而减少后面网络层的参数
		- 池化层通常接在卷积层后面.
		- 池化具体操作
			- 相当于在卷积层上架了一个窗口,但它不需要w,b这些参数
			- 窗口移动的stride也跟卷积层的窗口不一样.
			- 问 不同的(卷积层)特征图使用相同的池化操作?(比如对每个Feature map都采用最大池化?)	
		- 池化操作会丢失上层(卷积层)特征所在的**精准位置信息**,所幸的是可以保留**相对位置信息**,(在识别时,后者更重要)
		- 常用的池化方式
			- max-pooling
			- L2-pooling
			- .......
- **总体来看CNN的结构**
	- 输入层-->卷积层-->池化层-->输出层
	- ** 卷积层和池采样层**是实现卷积神经网络**特征提取功能的核心模块**
	- 低隐层 : 卷积层和池化层交替组成
	- 高隐层 : 全连接层,可以有多个类似传统MLP的隐藏层.第一个高隐层的输入: 由卷积,池化得到的Feature map;最后一层的输出层是一个分类器,可以支持**逻辑回归**,**SoftMax回归**,**SVM**等对输入图像进行分类.
	- 其中,池化层到输出层是**全连接的**,这和DNN是一样的.
- **训练神经网络**
	- 开始训练前,权重w的`小随机数`初始化
		- `小随机数` 用来保证网络不会因权值过大而进入饱和状态,从而导致训练失败
		- `不同` 保证网络可以正常的学习,(==若用相同的数,去初始化权值矩阵,则网络无能力学习==.why)
	- 训练算法跟BP算法差不多,采用梯度下降法最小化损失函数对网络中的权重参数逐层反向调节，通过频繁的迭代训练提高网络的精度
	
- 输入层神经元: 二维 ,跟DNN网络时候一样,数目没有改变,依旧是像素的个数
- 隐藏层神经元: 二维 经过卷积核操作后,其值组成的二维