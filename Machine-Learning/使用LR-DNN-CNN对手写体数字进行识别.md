使用LR-DNN-CNN对手写体数字进行识别
===

- **input** 5000个训练样本,每个样本是20\*20像素的灰度图像.
- **output** 0~9 共10类 (K分类问题)

***
#### **Logistic Regression**实现
- 使用逻辑回归模型中的**one-vs-all**模型(**一对余**)来构建一个多分类器.
- 一个**逻辑回归模型**其相当于一个**单层神经网络**:相当于**只有一个神经元**
- 一个**逻辑回归**只能分**两类**,**one-vs-all**模型的思想是`一对余`,K分类则需要构建K个逻辑回归模型,每个逻辑回归只针对某个特定的类.
- 从**one-vs-all**的输出探究:
	- 该模型输出的结果形式为:计算10个分类器中的每个分类器的最终权重，并将权重返回shape为(k, (n+1))数组，其中 n 是参数(theta)数量
	- 本例中:输出为(10,401)的矩阵.10行表示有10个逻辑回归模型,每行的401个数为最终学到的theta值.
	- 最终的预测矩阵 : **训练集**(或测试集)与**学到的参数**进行矩阵相乘 本例为(5000,401)*(10,401).T=(5000,10) 
	- 每一行有10个数值(0~1)之间,表示某个样本属于第i类(i=1,2,3,....,10)的概率,其中概率最大的预测这个样本属于这一类.
- **参数个数**
	- 相当于用了K个神经元.每个神经元来学习一个类,每个神经元的输入个数为(样本的特征个数+1)
	- 最终需要学到的参数有 10\*401 = 4010个参数(即本例的输出矩阵)
- 实验结果
> 实验测得 : accuracy = 94.46%
( 使用了scipy.optimize包里面的minimize来最小化 **J(θ)** )
- 预测的准确度不算太高,这是因为**Logistic Regression**不能形成更复杂的假设,因为它只是一个线性分类器.

#### **DNN**实现
- 深度神经网络**DNN**跟多层感知器**MLP**没啥区别?
- 这里使用2层的神经网络来训练.(即 一个**hidden layer**,一个**output layer**)
- 网络配置
	- input layer : (400+1)
	- hidden layer : (25+1)  个数由自己来决定
	- output layer : (10) 最终要分10类,所以输出层是10个神经元 
	- 有两个权重矩阵 theta1 : (25,401)   theta2 : (10,26)
- **参数个数**
	- 学到的参数theta(权重)个数 : 25\*401 + 10\*26 = 10285 个 (比LR要多很多)
	- 用到的神经元的个数为 26 + 10 = 36个 (隐藏层+输出层)
- 实验结果
> accuracy = 97.52%
(使用反向传播来获得梯度,使用了scipy.optimize包里面的minimize来最小化 **J(θ)**)
- 预测结果较LR有提升.

#### **CNN**实现



















